/**
 * @note This parser header file is generated from a template.
 * Any modifications to this file will be overwritten.
 * Please do not edit this file directly.
 *
 * @note 此文件是从模板生成的。
 * 任何对该文件的修改都将被覆盖。
 * 请不要直接编辑此文件。
 */

#ifndef __GRAMMAR_SYNTAX_H__
#define __GRAMMAR_SYNTAX_H__

#include "alioth/ast.h"
#include "annotation/syntax.h"
#include "assignment/syntax.h"


namespace grammar {

constexpr alioth::SymbolID LEAD = 1;
constexpr alioth::SymbolID LT = 2;
constexpr alioth::SymbolID GT = 3;
constexpr alioth::SymbolID UNION = 4;
constexpr alioth::SymbolID DEFINE = 5;
constexpr alioth::SymbolID IGNORE = 6;
constexpr alioth::SymbolID AT = 7;
constexpr alioth::SymbolID SEMICOLON = 8;
constexpr alioth::SymbolID COLON = 9;
constexpr alioth::SymbolID COMMA = 10;
constexpr alioth::SymbolID DOT = 11;
constexpr alioth::SymbolID UNFOLD = 12;
constexpr alioth::SymbolID LBRACE = 13;
constexpr alioth::SymbolID RBRACE = 14;
constexpr alioth::SymbolID LPAREN = 15;
constexpr alioth::SymbolID RPAREN = 16;
constexpr alioth::SymbolID LBRACKET = 17;
constexpr alioth::SymbolID RBRACKET = 18;
constexpr alioth::SymbolID EMPTY = 19;
constexpr alioth::SymbolID LANG = 20;
constexpr alioth::SymbolID IMPORT = 21;
constexpr alioth::SymbolID AS = 22;
constexpr alioth::SymbolID JNULL = 23;
constexpr alioth::SymbolID TRUE = 24;
constexpr alioth::SymbolID FALSE = 25;
constexpr alioth::SymbolID STRING = 26;
constexpr alioth::SymbolID NUMBER = 27;
constexpr alioth::SymbolID ID = 28;
constexpr alioth::SymbolID REGEX = 29;
constexpr alioth::SymbolID COMMENT = 30;
constexpr alioth::SymbolID SPACE = 31;

struct EmptyFormula; // SymbolID = 46; Accepts: [46]
struct Formula; // SymbolID = 45; Accepts: [45, 47]
struct Grammar; // SymbolID = 33; Accepts: [33]
struct Import; // SymbolID = 39; Accepts: [39]
struct Ntrm; // SymbolID = 43; Accepts: [43]
struct Symbol; // SymbolID = 48; Accepts: [48]
struct Term; // SymbolID = 40; Accepts: [40]


using Annotation = ::annotation::Annotation;
using Assignment = ::assignment::Assignment;


struct EmptyFormula {
  alioth::AST empty() const;
  alioth::AST node{};

  operator alioth::AST() const { return node; }
  operator bool() const { return node != nullptr; }
  alioth::ASTNode* operator->() const { return node.get(); }
  alioth::ASTNode& operator*() const { return *node; }
};

struct Formula {
  std::vector<Symbol> symbols() const;
  alioth::AST node{};

  operator alioth::AST() const { return node; }
  operator bool() const { return node != nullptr; }
  alioth::ASTNode* operator->() const { return node.get(); }
  alioth::ASTNode& operator*() const { return *node; }
};

struct Grammar {
  std::vector<Assignment> assignments() const;
  std::vector<Import> imports() const;
  alioth::AST lang() const;
  std::vector<Ntrm> ntrms() const;
  std::vector<Term> terms() const;
  alioth::AST node{};

  operator alioth::AST() const { return node; }
  operator bool() const { return node != nullptr; }
  alioth::ASTNode* operator->() const { return node.get(); }
  alioth::ASTNode& operator*() const { return *node; }
};

struct Import {
  alioth::AST alias() const;
  alioth::AST from() const;
  alioth::AST node{};

  operator alioth::AST() const { return node; }
  operator bool() const { return node != nullptr; }
  alioth::ASTNode* operator->() const { return node.get(); }
  alioth::ASTNode& operator*() const { return *node; }
};

struct Ntrm {
  alioth::AST form() const;
  std::vector<alioth::AST> formulas() const;
  alioth::AST name() const;
  alioth::AST node{};

  operator alioth::AST() const { return node; }
  operator bool() const { return node != nullptr; }
  alioth::ASTNode* operator->() const { return node.get(); }
  alioth::ASTNode& operator*() const { return *node; }
};

struct Symbol {
  alioth::AST attr() const;
  alioth::AST name() const;
  alioth::AST optional() const;
  alioth::AST node{};

  operator alioth::AST() const { return node; }
  operator bool() const { return node != nullptr; }
  alioth::ASTNode* operator->() const { return node.get(); }
  alioth::ASTNode& operator*() const { return *node; }
};

struct Term {
  std::vector<alioth::AST> contexts() const;
  alioth::AST ignore() const;
  alioth::AST name() const;
  alioth::AST regex() const;
  alioth::AST node{};

  operator alioth::AST() const { return node; }
  operator bool() const { return node != nullptr; }
  alioth::ASTNode* operator->() const { return node.get(); }
  alioth::ASTNode& operator*() const { return *node; }
};


}

namespace alioth {
template<>
inline grammar::EmptyFormula alioth::ASTNode::As<grammar::EmptyFormula>() {
  switch(id) {
    case 46:
    return grammar::EmptyFormula{shared_from_this()};
  default:
    return {};
  }
}

template<>
inline grammar::Formula alioth::ASTNode::As<grammar::Formula>() {
  switch(id) {
    case 45:case 47:
    return grammar::Formula{shared_from_this()};
  default:
    return {};
  }
}

template<>
inline grammar::Grammar alioth::ASTNode::As<grammar::Grammar>() {
  switch(id) {
    case 33:
    return grammar::Grammar{shared_from_this()};
  default:
    return {};
  }
}

template<>
inline grammar::Import alioth::ASTNode::As<grammar::Import>() {
  switch(id) {
    case 39:
    return grammar::Import{shared_from_this()};
  default:
    return {};
  }
}

template<>
inline grammar::Ntrm alioth::ASTNode::As<grammar::Ntrm>() {
  switch(id) {
    case 43:
    return grammar::Ntrm{shared_from_this()};
  default:
    return {};
  }
}

template<>
inline grammar::Symbol alioth::ASTNode::As<grammar::Symbol>() {
  switch(id) {
    case 48:
    return grammar::Symbol{shared_from_this()};
  default:
    return {};
  }
}

template<>
inline grammar::Term alioth::ASTNode::As<grammar::Term>() {
  switch(id) {
    case 40:
    return grammar::Term{shared_from_this()};
  default:
    return {};
  }
}


template<>
inline Syntax SyntaxOf<grammar::Grammar>() {
  static auto syntax = []{
    using namespace nlohmann;
    auto lex = Lexicon::Builder("grammar");
    lex.Define("LEAD", R"(->)"_regex);
    lex.Annotate("LEAD", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("LT", R"(<)"_regex);
    lex.Annotate("LT", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("GT", R"(>)"_regex);
    lex.Annotate("GT", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("UNION", R"(\|)"_regex);
    lex.Annotate("UNION", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("DEFINE", R"(=)"_regex);
    lex.Annotate("DEFINE", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("IGNORE", R"(\?)"_regex);
    lex.Annotate("IGNORE", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("AT", R"(@)"_regex);
    lex.Annotate("AT", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("SEMICOLON", R"(;)"_regex);
    lex.Annotate("SEMICOLON", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("COLON", R"(:)"_regex);
    lex.Annotate("COLON", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("COMMA", R"(,)"_regex);
    lex.Annotate("COMMA", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("DOT", R"(\.)"_regex);
    lex.Annotate("DOT", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("UNFOLD", R"(\.\.\.)"_regex);
    lex.Annotate("UNFOLD", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("LBRACE", R"({)"_regex);
    lex.Annotate("LBRACE", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("RBRACE", R"(})"_regex);
    lex.Annotate("RBRACE", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("LPAREN", R"(\()"_regex);
    lex.Annotate("LPAREN", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("RPAREN", R"(\))"_regex);
    lex.Annotate("RPAREN", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("LBRACKET", R"(\[)"_regex);
    lex.Annotate("LBRACKET", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("RBRACKET", R"(])"_regex);
    lex.Annotate("RBRACKET", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("EMPTY", R"(%empty)"_regex);
    lex.Annotate("EMPTY", "tokenize", R"({"type":"keyword"})"_json);
    
    lex.Define("LANG", R"(lang)"_regex, { "keyword",  });
    lex.Annotate("LANG", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("IMPORT", R"(import)"_regex, { "keyword",  });
    lex.Annotate("IMPORT", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("AS", R"(as)"_regex, { "keyword",  });
    lex.Annotate("AS", "tokenize", R"({"type":"operator"})"_json);
    
    lex.Define("JNULL", R"(null)"_regex, { "json",  });
    lex.Annotate("JNULL", "tokenize", R"({"type":"keyword"})"_json);
    
    lex.Define("TRUE", R"(true)"_regex, { "json",  });
    lex.Annotate("TRUE", "tokenize", R"({"type":"keyword"})"_json);
    
    lex.Define("FALSE", R"(false)"_regex, { "json",  });
    lex.Annotate("FALSE", "tokenize", R"({"type":"keyword"})"_json);
    
    lex.Define("STRING", R"(\"([^\"\n\\]|\\[^\n])*\")"_regex, { "json",  });
    lex.Annotate("STRING", "tokenize", R"({"type":"string"})"_json);
    
    lex.Define("NUMBER", R"(-?(0|[1-9]\d*)(\.\d+)?([eE][+-]?\d+)?)"_regex, { "json",  });
    lex.Annotate("NUMBER", "tokenize", R"({"type":"number"})"_json);
    
    lex.Define("ID", R"([a-zA-Z_]\w*)"_regex);
    
    lex.Define("REGEX", R"(\/([^\\\/]|\\[^\n])+\/)"_regex);
    lex.Annotate("REGEX", "tokenize", R"({"type":"regexp"})"_json);
    
    lex.Define("COMMENT", R"(#[^\n]*\n)"_regex);
    lex.Annotate("COMMENT", "tokenize", R"({"type":"comment"})"_json);
    
    lex.Define("SPACE", R"(\s+)"_regex);
    
    
    auto syntax = Syntactic::Builder(lex.Build());
    syntax.Ignore("COMMENT");
    syntax.Ignore("SPACE");
    
    syntax.Import(::alioth::Syntax<::annotation::Annotation>(), "annotation");
    syntax.Import(::alioth::Syntax<::assignment::Assignment>(), "assignment");
    syntax.Formula("grammar").Symbol("LANG").Symbol("COLON").Symbol("ID", "lang").Commit();
    syntax.Formula("grammar").Symbol("LANG").Symbol("COLON").Symbol("ID", "lang").Symbol("imports", "...").Commit();
    syntax.Formula("grammar").Symbol("LANG").Symbol("COLON").Symbol("ID", "lang").Symbol("terms", "...").Commit();
    syntax.Formula("grammar").Symbol("LANG").Symbol("COLON").Symbol("ID", "lang").Symbol("imports", "...").Symbol("terms", "...").Commit();
    syntax.Formula("grammar").Symbol("LANG").Symbol("COLON").Symbol("ID", "lang").Symbol("ntrms", "...").Commit();
    syntax.Formula("grammar").Symbol("LANG").Symbol("COLON").Symbol("ID", "lang").Symbol("imports", "...").Symbol("ntrms", "...").Commit();
    syntax.Formula("grammar").Symbol("LANG").Symbol("COLON").Symbol("ID", "lang").Symbol("terms", "...").Symbol("ntrms", "...").Commit();
    syntax.Formula("grammar").Symbol("LANG").Symbol("COLON").Symbol("ID", "lang").Symbol("imports", "...").Symbol("terms", "...").Symbol("ntrms", "...").Commit();
    syntax.Formula("imports").Symbol("import", "imports").Commit();
    syntax.Formula("imports").Symbol("imports", "...").Symbol("import", "imports").Commit();
    syntax.Formula("import").Symbol("IMPORT").Symbol("STRING", "from").Commit();
    syntax.Formula("import").Symbol("IMPORT").Symbol("STRING", "from").Symbol("AS").Symbol("ID", "alias").Commit();
    syntax.Formula("terms").Symbol("term", "terms").Commit();
    syntax.Formula("terms").Symbol("terms", "...").Symbol("term", "terms").Commit();
    syntax.Formula("terms").Symbol("terms", "...").Symbol("assignment", "assignments").Commit();
    syntax.Formula("term").Symbol("ID", "name").Symbol("DEFINE").Symbol("REGEX", "regex")
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("term").Symbol("ID", "name").Symbol("contexts", "...").Symbol("DEFINE").Symbol("REGEX", "regex")
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("term").Symbol("ID", "name").Symbol("IGNORE", "ignore").Symbol("DEFINE").Symbol("REGEX", "regex")
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("term").Symbol("ID", "name").Symbol("contexts", "...").Symbol("IGNORE", "ignore").Symbol("DEFINE").Symbol("REGEX", "regex")
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("term").Symbol("ID", "name").Symbol("DEFINE").Symbol("REGEX", "regex").Symbol("annotation", "...")
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("term").Symbol("ID", "name").Symbol("contexts", "...").Symbol("DEFINE").Symbol("REGEX", "regex").Symbol("annotation", "...")
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("term").Symbol("ID", "name").Symbol("IGNORE", "ignore").Symbol("DEFINE").Symbol("REGEX", "regex").Symbol("annotation", "...")
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("term").Symbol("ID", "name").Symbol("contexts", "...").Symbol("IGNORE", "ignore").Symbol("DEFINE").Symbol("REGEX", "regex").Symbol("annotation", "...")
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("contexts").Symbol("LT").Symbol("context_list", "...").Symbol("GT").Commit();
    syntax.Formula("context_list").Symbol("ID", "contexts")
      .Annotate("contexts", "tokenize", R"({"modifier":["definition"],"type":"decorator"})"_json)
      .Commit();
    syntax.Formula("context_list").Symbol("context_list", "...").Symbol("COMMA").Symbol("ID", "contexts")
      .Annotate("contexts", "tokenize", R"({"modifier":["definition"],"type":"decorator"})"_json)
      .Commit();
    syntax.Formula("ntrms").Symbol("ntrm", "ntrms").Commit();
    syntax.Formula("ntrms").Symbol("ntrms", "...").Symbol("ntrm", "ntrms").Commit();
    syntax.Formula("ntrms").Symbol("ntrms", "...").Symbol("assignment", "assignments").Commit();
    syntax.Formula("ntrm").Symbol("ID", "name").Symbol("LEAD").Symbol("formula_group", "...").Symbol("SEMICOLON")
      .Annotate("form", "tokenize", R"({"modifier":["definition"],"type":"decorator"})"_json)
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("ntrm").Symbol("ID", "name").Symbol("DOT").Symbol("ID", "form").Symbol("LEAD").Symbol("formula_group", "...").Symbol("SEMICOLON")
      .Annotate("form", "tokenize", R"({"modifier":["definition"],"type":"decorator"})"_json)
      .Annotate("name", "tokenize", R"({"modifier":["definition"],"type":"class"})"_json)
      .Commit();
    syntax.Formula("formula_group").Symbol("formula", "formulas").Commit();
    syntax.Formula("formula_group").Symbol("empty_formula", "formulas").Commit();
    syntax.Formula("formula_group").Symbol("formula_group", "...").Symbol("UNION").Symbol("formula", "formulas").Commit();
    syntax.Formula("formula_group").Symbol("formula_group", "...").Symbol("UNION").Symbol("empty_formula", "formulas").Commit();
    syntax.Formula("formula").Symbol("formula_body", "...").Commit();
    syntax.Formula("formula").Symbol("formula_body", "...").Symbol("annotation", "...").Commit();
    syntax.Formula("formula_body").Symbol("symbol", "symbols").Commit();
    syntax.Formula("formula_body").Symbol("formula_body", "...").Symbol("symbol", "symbols").Commit();
    syntax.Formula("empty_formula").Symbol("EMPTY", "empty").Commit();
    syntax.Formula("symbol").Symbol("ID", "name")
      .Annotate("attr", "tokenize", R"({"modifier":["definition"],"type":"property"})"_json)
      .Annotate("name", "tokenize", R"({"type":"string"})"_json)
      .Commit();
    syntax.Formula("symbol").Symbol("ID", "name").Symbol("IGNORE", "optional")
      .Annotate("attr", "tokenize", R"({"modifier":["definition"],"type":"property"})"_json)
      .Annotate("name", "tokenize", R"({"type":"string"})"_json)
      .Commit();
    syntax.Formula("symbol").Symbol("ID", "name").Symbol("AT").Symbol("ID", "attr")
      .Annotate("attr", "tokenize", R"({"modifier":["definition"],"type":"property"})"_json)
      .Annotate("name", "tokenize", R"({"type":"string"})"_json)
      .Commit();
    syntax.Formula("symbol").Symbol("ID", "name").Symbol("IGNORE", "optional").Symbol("AT").Symbol("ID", "attr")
      .Annotate("attr", "tokenize", R"({"modifier":["definition"],"type":"property"})"_json)
      .Annotate("name", "tokenize", R"({"type":"string"})"_json)
      .Commit();
    syntax.Formula("symbol").Symbol("UNFOLD", "attr").Symbol("ID", "name")
      .Annotate("attr", "tokenize", R"({"modifier":["definition"],"type":"property"})"_json)
      .Annotate("name", "tokenize", R"({"type":"string"})"_json)
      .Commit();
    syntax.Formula("symbol").Symbol("UNFOLD", "attr").Symbol("ID", "name").Symbol("IGNORE", "optional")
      .Annotate("attr", "tokenize", R"({"modifier":["definition"],"type":"property"})"_json)
      .Annotate("name", "tokenize", R"({"type":"string"})"_json)
      .Commit();
    
    return syntax.Build();
  }();

  return syntax;
}
}

namespace grammar {

inline alioth::AST EmptyFormula::empty() const { 
  return node->Attr("empty");
}

inline std::vector<Symbol> Formula::symbols() const { 
  return alioth::generic::collect<alioth::generic::multiple>(
    node->Attrs("symbols"), 
    [](auto n) {
      return alioth::ViewOf<Symbol>(n);
    }
  );
}

inline std::vector<Assignment> Grammar::assignments() const { 
  return alioth::generic::collect<alioth::generic::multiple>(
    node->Attrs("assignments"), 
    [](auto n) {
      return n->template As<Assignment>();
    }
  );
}
inline std::vector<Import> Grammar::imports() const { 
  return alioth::generic::collect<alioth::generic::multiple>(
    node->Attrs("imports"), 
    [](auto n) {
      return alioth::ViewOf<Import>(n);
    }
  );
}
inline alioth::AST Grammar::lang() const { 
  return node->Attr("lang");
}
inline std::vector<Ntrm> Grammar::ntrms() const { 
  return alioth::generic::collect<alioth::generic::multiple>(
    node->Attrs("ntrms"), 
    [](auto n) {
      return alioth::ViewOf<Ntrm>(n);
    }
  );
}
inline std::vector<Term> Grammar::terms() const { 
  return alioth::generic::collect<alioth::generic::multiple>(
    node->Attrs("terms"), 
    [](auto n) {
      return alioth::ViewOf<Term>(n);
    }
  );
}

inline alioth::AST Import::alias() const { 
  return node->Attr("alias");
}
inline alioth::AST Import::from() const { 
  return node->Attr("from");
}

inline alioth::AST Ntrm::form() const { 
  return node->Attr("form");
}
inline std::vector<alioth::AST> Ntrm::formulas() const { 
  return node->Attrs("formulas"); 
}
inline alioth::AST Ntrm::name() const { 
  return node->Attr("name");
}

inline alioth::AST Symbol::attr() const { 
  return node->Attr("attr");
}
inline alioth::AST Symbol::name() const { 
  return node->Attr("name");
}
inline alioth::AST Symbol::optional() const { 
  return node->Attr("optional");
}

inline std::vector<alioth::AST> Term::contexts() const { 
  return node->Attrs("contexts"); 
}
inline alioth::AST Term::ignore() const { 
  return node->Attr("ignore");
}
inline alioth::AST Term::name() const { 
  return node->Attr("name");
}
inline alioth::AST Term::regex() const { 
  return node->Attr("regex");
}



}

#endif